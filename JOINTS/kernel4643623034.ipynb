{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import pustaka"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nfrom datetime import datetime\n\npd.set_option('display.max_columns', None)  ","execution_count":229,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc_train = pd.read_csv(\"../input/train_data.csv\")\ntest = pd.read_csv(\"../input/test_data.csv\")\npreproc_train.head()","execution_count":230,"outputs":[{"output_type":"execute_result","execution_count":230,"data":{"text/plain":"   id word-1  word-2  word-3  word-4 word-5  word-6  word-7 word-8 word-9  \\\n0   1      5     0.0     0.0     0.0      0     0.0     0.0      1      0   \n1   2    117     1.0     4.0     3.0    NaN     5.0     2.0      9     19   \n2   3     30     1.0     1.0     5.0      8     3.0     2.0      2      2   \n3   4      3     0.0     0.0     0.0      0     0.0     0.0      0      1   \n4   5    263     6.0     8.0     8.0    NaN    26.0     1.0     21     35   \n\n  word-10 word-11  word-12 word-13 word-14  word-15  word-16 word-17 word-18  \\\n0       1       1      0.0       0       1      0.0      0.0       0       0   \n1       6       1      2.0       3       9     15.0      4.0      10       9   \n2       1       1      1.0       0       1      7.0      1.0       1       1   \n3       0       0      0.0       0       1      1.0      0.0       0       0   \n4      10       5      9.0       4      14     42.0     13.0      15       6   \n\n  word-19  word-20  word-21 word-22  word-23 word-24  word-25  word-26  \\\n0       1      0.0      0.0       0      0.0       0      0.0      0.0   \n1      79      1.0      0.0      19     17.0       5     25.0      0.0   \n2      17      1.0      1.0       7      1.0       0     11.0      2.0   \n3       1      0.0      0.0       0      0.0       0      0.0      0.0   \n4     NaN     20.0      4.0      86     19.0      25     18.0      5.0   \n\n  word-27 word-28  word-29 word-30  word-31  word-32 word-33  word-34 word-35  \\\n0       0       0      1.0       0      0.0      0.0       0      0.0       0   \n1       8       2      NaN       2      1.0      3.0      13      9.0      15   \n2       1       1      2.0       0      0.0      1.0       5      1.0       1   \n3       1       0      0.0       0      0.0      0.0     NaN      0.0       0   \n4      14      24     56.0      20      6.0     10.0     NaN      5.0      49   \n\n   word-36  word-37 word-38 word-39 word-40  Result  \n0      0.0      0.0       0       0       0       0  \n1      3.0      NaN      13       3       1       0  \n2      1.0      0.0       0       0       0       0  \n3      0.0      0.0     NaN       0       0       0  \n4     26.0     24.0      23       4      24       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>word-1</th>\n      <th>word-2</th>\n      <th>word-3</th>\n      <th>word-4</th>\n      <th>word-5</th>\n      <th>word-6</th>\n      <th>word-7</th>\n      <th>word-8</th>\n      <th>word-9</th>\n      <th>word-10</th>\n      <th>word-11</th>\n      <th>word-12</th>\n      <th>word-13</th>\n      <th>word-14</th>\n      <th>word-15</th>\n      <th>word-16</th>\n      <th>word-17</th>\n      <th>word-18</th>\n      <th>word-19</th>\n      <th>word-20</th>\n      <th>word-21</th>\n      <th>word-22</th>\n      <th>word-23</th>\n      <th>word-24</th>\n      <th>word-25</th>\n      <th>word-26</th>\n      <th>word-27</th>\n      <th>word-28</th>\n      <th>word-29</th>\n      <th>word-30</th>\n      <th>word-31</th>\n      <th>word-32</th>\n      <th>word-33</th>\n      <th>word-34</th>\n      <th>word-35</th>\n      <th>word-36</th>\n      <th>word-37</th>\n      <th>word-38</th>\n      <th>word-39</th>\n      <th>word-40</th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>117</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>9</td>\n      <td>19</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>9</td>\n      <td>15.0</td>\n      <td>4.0</td>\n      <td>10</td>\n      <td>9</td>\n      <td>79</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>19</td>\n      <td>17.0</td>\n      <td>5</td>\n      <td>25.0</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>13</td>\n      <td>9.0</td>\n      <td>15</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>13</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>30</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>8</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>17</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>11.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>263</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>21</td>\n      <td>35</td>\n      <td>10</td>\n      <td>5</td>\n      <td>9.0</td>\n      <td>4</td>\n      <td>14</td>\n      <td>42.0</td>\n      <td>13.0</td>\n      <td>15</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>20.0</td>\n      <td>4.0</td>\n      <td>86</td>\n      <td>19.0</td>\n      <td>25</td>\n      <td>18.0</td>\n      <td>5.0</td>\n      <td>14</td>\n      <td>24</td>\n      <td>56.0</td>\n      <td>20</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>49</td>\n      <td>26.0</td>\n      <td>24.0</td>\n      <td>23</td>\n      <td>4</td>\n      <td>24</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Cek jumlah data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(\"train length: \",len(preproc_train))\nprint(\"test length: \",len(test))","execution_count":231,"outputs":[{"output_type":"stream","text":"train length:  3620\ntest length:  1552\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Cek perbandingan kelas "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.countplot(x='Result', data=preproc_train)\npreproc_train.Result.value_counts()\n","execution_count":232,"outputs":[{"output_type":"execute_result","execution_count":232,"data":{"text/plain":"0    2574\n1    1046\nName: Result, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"# Cek variabilitas data"},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc_train.describe()","execution_count":233,"outputs":[{"output_type":"execute_result","execution_count":233,"data":{"text/plain":"                id       word-2       word-3       word-4       word-6  \\\ncount  3620.000000  3604.000000  3606.000000  3597.000000  3597.000000   \nmean   1810.500000     1.752775     1.298392     1.926606     3.046150   \nstd    1045.148315     3.751389     2.766599     4.131414     6.061796   \nmin       1.000000     0.000000     0.000000    -1.000000     0.000000   \n25%     905.750000     0.000000     0.000000     0.000000     0.000000   \n50%    1810.500000     1.000000     0.000000     1.000000     1.000000   \n75%    2715.250000     2.000000     1.000000     2.000000     3.000000   \nmax    3620.000000    81.000000    66.000000   117.000000    89.000000   \n\n            word-7      word-12      word-15      word-16      word-20  \\\ncount  3600.000000  3600.000000  3596.000000  3609.000000  3606.000000   \nmean      1.410000     1.290278     9.063404     1.435578     1.513034   \nstd       2.590879     2.742220    14.611916     2.667310     4.364428   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     1.000000     0.000000     0.000000   \n50%       0.000000     0.000000     4.000000     0.000000     0.000000   \n75%       2.000000     1.000000    10.000000     2.000000     2.000000   \nmax      31.000000    56.000000   275.000000    39.000000   123.000000   \n\n           word-21      word-23      word-25      word-26      word-29  \\\ncount  3609.000000  3605.000000  3603.000000  3607.000000  3604.000000   \nmean      1.259352     5.324827     5.262559     1.068755    10.971976   \nstd       2.492630     9.020088     8.098156     3.071660    18.276807   \nmin       0.000000     0.000000    -9.000000     0.000000     0.000000   \n25%       0.000000     0.000000     1.000000     0.000000     2.000000   \n50%       0.000000     2.000000     3.000000     0.000000     5.000000   \n75%       2.000000     6.000000     6.000000     1.000000    12.000000   \nmax      43.000000   111.000000   108.000000    39.000000   302.000000   \n\n           word-31      word-32      word-34        word-36      word-37  \\\ncount  3609.000000  3610.000000  3609.000000    3607.000000  3606.000000   \nmean      1.319202     1.772853     1.372125      37.739950     3.247920   \nstd       3.386842     3.400314     2.794576    2163.622051     6.087762   \nmin       0.000000     0.000000     0.000000       0.000000     0.000000   \n25%       0.000000     0.000000     0.000000       0.000000     0.000000   \n50%       0.000000     1.000000     0.000000       0.000000     1.000000   \n75%       1.000000     2.000000     2.000000       2.000000     3.000000   \nmax      57.000000    66.000000    33.000000  129945.000000   108.000000   \n\n            Result  \ncount  3620.000000  \nmean      0.288950  \nstd       0.453337  \nmin       0.000000  \n25%       0.000000  \n50%       0.000000  \n75%       1.000000  \nmax       1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>word-2</th>\n      <th>word-3</th>\n      <th>word-4</th>\n      <th>word-6</th>\n      <th>word-7</th>\n      <th>word-12</th>\n      <th>word-15</th>\n      <th>word-16</th>\n      <th>word-20</th>\n      <th>word-21</th>\n      <th>word-23</th>\n      <th>word-25</th>\n      <th>word-26</th>\n      <th>word-29</th>\n      <th>word-31</th>\n      <th>word-32</th>\n      <th>word-34</th>\n      <th>word-36</th>\n      <th>word-37</th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3620.000000</td>\n      <td>3604.000000</td>\n      <td>3606.000000</td>\n      <td>3597.000000</td>\n      <td>3597.000000</td>\n      <td>3600.000000</td>\n      <td>3600.000000</td>\n      <td>3596.000000</td>\n      <td>3609.000000</td>\n      <td>3606.000000</td>\n      <td>3609.000000</td>\n      <td>3605.000000</td>\n      <td>3603.000000</td>\n      <td>3607.000000</td>\n      <td>3604.000000</td>\n      <td>3609.000000</td>\n      <td>3610.000000</td>\n      <td>3609.000000</td>\n      <td>3607.000000</td>\n      <td>3606.000000</td>\n      <td>3620.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1810.500000</td>\n      <td>1.752775</td>\n      <td>1.298392</td>\n      <td>1.926606</td>\n      <td>3.046150</td>\n      <td>1.410000</td>\n      <td>1.290278</td>\n      <td>9.063404</td>\n      <td>1.435578</td>\n      <td>1.513034</td>\n      <td>1.259352</td>\n      <td>5.324827</td>\n      <td>5.262559</td>\n      <td>1.068755</td>\n      <td>10.971976</td>\n      <td>1.319202</td>\n      <td>1.772853</td>\n      <td>1.372125</td>\n      <td>37.739950</td>\n      <td>3.247920</td>\n      <td>0.288950</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1045.148315</td>\n      <td>3.751389</td>\n      <td>2.766599</td>\n      <td>4.131414</td>\n      <td>6.061796</td>\n      <td>2.590879</td>\n      <td>2.742220</td>\n      <td>14.611916</td>\n      <td>2.667310</td>\n      <td>4.364428</td>\n      <td>2.492630</td>\n      <td>9.020088</td>\n      <td>8.098156</td>\n      <td>3.071660</td>\n      <td>18.276807</td>\n      <td>3.386842</td>\n      <td>3.400314</td>\n      <td>2.794576</td>\n      <td>2163.622051</td>\n      <td>6.087762</td>\n      <td>0.453337</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-9.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>905.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1810.500000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2715.250000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>10.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>1.000000</td>\n      <td>12.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3620.000000</td>\n      <td>81.000000</td>\n      <td>66.000000</td>\n      <td>117.000000</td>\n      <td>89.000000</td>\n      <td>31.000000</td>\n      <td>56.000000</td>\n      <td>275.000000</td>\n      <td>39.000000</td>\n      <td>123.000000</td>\n      <td>43.000000</td>\n      <td>111.000000</td>\n      <td>108.000000</td>\n      <td>39.000000</td>\n      <td>302.000000</td>\n      <td>57.000000</td>\n      <td>66.000000</td>\n      <td>33.000000</td>\n      <td>129945.000000</td>\n      <td>108.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Cek apakah ada data dengan id duplikat"},{"metadata":{"trusted":true},"cell_type":"code","source":"dup = preproc_train[\"id\"].duplicated() \ndup.value_counts()","execution_count":234,"outputs":[{"output_type":"execute_result","execution_count":234,"data":{"text/plain":"False    3620\nName: id, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Data id tidak ada duplikat, kolom id bisa di drop"},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc_train.drop('id', axis=1, inplace=True)\npreproc_train.head()","execution_count":235,"outputs":[{"output_type":"execute_result","execution_count":235,"data":{"text/plain":"  word-1  word-2  word-3  word-4 word-5  word-6  word-7 word-8 word-9 word-10  \\\n0      5     0.0     0.0     0.0      0     0.0     0.0      1      0       1   \n1    117     1.0     4.0     3.0    NaN     5.0     2.0      9     19       6   \n2     30     1.0     1.0     5.0      8     3.0     2.0      2      2       1   \n3      3     0.0     0.0     0.0      0     0.0     0.0      0      1       0   \n4    263     6.0     8.0     8.0    NaN    26.0     1.0     21     35      10   \n\n  word-11  word-12 word-13 word-14  word-15  word-16 word-17 word-18 word-19  \\\n0       1      0.0       0       1      0.0      0.0       0       0       1   \n1       1      2.0       3       9     15.0      4.0      10       9      79   \n2       1      1.0       0       1      7.0      1.0       1       1      17   \n3       0      0.0       0       1      1.0      0.0       0       0       1   \n4       5      9.0       4      14     42.0     13.0      15       6     NaN   \n\n   word-20  word-21 word-22  word-23 word-24  word-25  word-26 word-27  \\\n0      0.0      0.0       0      0.0       0      0.0      0.0       0   \n1      1.0      0.0      19     17.0       5     25.0      0.0       8   \n2      1.0      1.0       7      1.0       0     11.0      2.0       1   \n3      0.0      0.0       0      0.0       0      0.0      0.0       1   \n4     20.0      4.0      86     19.0      25     18.0      5.0      14   \n\n  word-28  word-29 word-30  word-31  word-32 word-33  word-34 word-35  \\\n0       0      1.0       0      0.0      0.0       0      0.0       0   \n1       2      NaN       2      1.0      3.0      13      9.0      15   \n2       1      2.0       0      0.0      1.0       5      1.0       1   \n3       0      0.0       0      0.0      0.0     NaN      0.0       0   \n4      24     56.0      20      6.0     10.0     NaN      5.0      49   \n\n   word-36  word-37 word-38 word-39 word-40  Result  \n0      0.0      0.0       0       0       0       0  \n1      3.0      NaN      13       3       1       0  \n2      1.0      0.0       0       0       0       0  \n3      0.0      0.0     NaN       0       0       0  \n4     26.0     24.0      23       4      24       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word-1</th>\n      <th>word-2</th>\n      <th>word-3</th>\n      <th>word-4</th>\n      <th>word-5</th>\n      <th>word-6</th>\n      <th>word-7</th>\n      <th>word-8</th>\n      <th>word-9</th>\n      <th>word-10</th>\n      <th>word-11</th>\n      <th>word-12</th>\n      <th>word-13</th>\n      <th>word-14</th>\n      <th>word-15</th>\n      <th>word-16</th>\n      <th>word-17</th>\n      <th>word-18</th>\n      <th>word-19</th>\n      <th>word-20</th>\n      <th>word-21</th>\n      <th>word-22</th>\n      <th>word-23</th>\n      <th>word-24</th>\n      <th>word-25</th>\n      <th>word-26</th>\n      <th>word-27</th>\n      <th>word-28</th>\n      <th>word-29</th>\n      <th>word-30</th>\n      <th>word-31</th>\n      <th>word-32</th>\n      <th>word-33</th>\n      <th>word-34</th>\n      <th>word-35</th>\n      <th>word-36</th>\n      <th>word-37</th>\n      <th>word-38</th>\n      <th>word-39</th>\n      <th>word-40</th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>117</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>9</td>\n      <td>19</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>9</td>\n      <td>15.0</td>\n      <td>4.0</td>\n      <td>10</td>\n      <td>9</td>\n      <td>79</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>19</td>\n      <td>17.0</td>\n      <td>5</td>\n      <td>25.0</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>13</td>\n      <td>9.0</td>\n      <td>15</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>13</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>8</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>17</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>11.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>263</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>21</td>\n      <td>35</td>\n      <td>10</td>\n      <td>5</td>\n      <td>9.0</td>\n      <td>4</td>\n      <td>14</td>\n      <td>42.0</td>\n      <td>13.0</td>\n      <td>15</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>20.0</td>\n      <td>4.0</td>\n      <td>86</td>\n      <td>19.0</td>\n      <td>25</td>\n      <td>18.0</td>\n      <td>5.0</td>\n      <td>14</td>\n      <td>24</td>\n      <td>56.0</td>\n      <td>20</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>49</td>\n      <td>26.0</td>\n      <td>24.0</td>\n      <td>23</td>\n      <td>4</td>\n      <td>24</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Ubah seluruh kolom ke bentuk float, catat mean untuk setiap kolom"},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc_train.apply(pd.to_numeric, errors='ignore', downcast='float')\nfloat_train = preproc_train.copy()","execution_count":236,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_list = []\nfor column in float_train:\n    if(column != 'Result'):\n        sum = 0\n        count = 0\n        for i in range(len(float_train[column])):\n                try:\n                    temp = float(float_train[column][i])\n                    if(not np.isnan(temp)):\n                        sum = sum + temp\n                        count = count + 1\n                except:\n                    pass        \n        #print(sum, count, sum/count)\n        mean_list.append(sum/count)\n","execution_count":237,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nmean_dict = {}\nfor column in preproc_train:\n    if(column != 'Result'):\n        dict = {column:  str(mean_list[i])}\n        mean_dict.update(dict) \n        i = i + 1\n\nmean_dict","execution_count":238,"outputs":[{"output_type":"execute_result","execution_count":238,"data":{"text/plain":"{'word-1': '218.46064301552107',\n 'word-2': '1.7527746947835738',\n 'word-3': '1.2983915696062118',\n 'word-4': '1.926605504587156',\n 'word-5': '9.576398552741441',\n 'word-6': '3.046149569085349',\n 'word-7': '1.41',\n 'word-8': '4.760011123470523',\n 'word-9': '6.859375',\n 'word-10': '3.2464573492636846',\n 'word-11': '2.1949458483754514',\n 'word-12': '1.2902777777777779',\n 'word-13': '1.3514190317195325',\n 'word-14': '3.083471991125901',\n 'word-15': '9.063403781979977',\n 'word-16': '1.4355777223607649',\n 'word-17': '2.9963868816008894',\n 'word-18': '1.4855475264035576',\n 'word-19': '473.7041072925398',\n 'word-20': '1.5130338325013866',\n 'word-21': '1.259351620947631',\n 'word-22': '8717.867516629713',\n 'word-23': '5.324826629680999',\n 'word-24': '4.5337950138504155',\n 'word-25': '5.26255897862892',\n 'word-26': '1.0687551982256722',\n 'word-27': '2.9603658536585367',\n 'word-28': '2.6310975609756095',\n 'word-29': '10.971975582685905',\n 'word-30': '1.5544142143253747',\n 'word-31': '1.319201995012469',\n 'word-32': '1.7728531855955678',\n 'word-33': '6.54181717143651',\n 'word-34': '1.372125242449432',\n 'word-35': '6.210555555555556',\n 'word-36': '37.73995009703354',\n 'word-37': '3.2479201331114806',\n 'word-38': '116.54616024397006',\n 'word-39': '1.9919734292831441',\n 'word-40': '2.4788498755875032'}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Lihat data yang mengandung symbol "},{"metadata":{"trusted":true},"cell_type":"code","source":"out_char = []\npreproc_train = float_train.copy()\npreproc_train = preproc_train.astype('str')\nfor column in preproc_train:\n    if(column != 'Result'):\n        preproc_train[column] = preproc_train[column].apply(lambda row: re.sub(\"[0-9]\", \"\", row))\n        out_char = out_char + preproc_train[column].unique().tolist()\n\nout_set = set(out_char)\nprint(out_set)\n","execution_count":239,"outputs":[{"output_type":"stream","text":"{'', '\\\\', 'a', '+F', '-.', 'nan', '.', 'h', '`', '['}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<b> '' -> data kosong hasil sub regex angka integer, <br>\n'+F', -> simbol tidak berarti, <br>\n'h',  -> simbol tidak berarti, <br>\n'\\\\',  -> simbol tidak berarti, <br>\n'[', -> simbol tidak berarti, <br>\n'-.', -> simbol bilangan negatif, <br>\n'.', -> simbol tanda desimal, <br>\n'a',  -> simbol tidak berarti, <br>\n'`', -> simbol tidak berarti, <br>\n'nan' -> simbol NaN, </b>"},{"metadata":{},"cell_type":"markdown","source":"# Bersihkan data yang mengandung symbol dan NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc_train = float_train.copy()\npreproc_train = preproc_train.astype('str')\n\nfor column in preproc_train:\n    if(column != 'Result'):\n        preproc_train[column] = preproc_train[column].apply(lambda row: str(mean_dict[column]) if row == \"4+F2185\" else row)\n        preproc_train[column] = preproc_train[column].apply(lambda row: re.sub(\"^()$\", str(mean_dict[column]), row))\n        preproc_train[column] = preproc_train[column].apply(lambda row: re.sub(\"[h]\", str(mean_dict[column]), row))\n        preproc_train[column] = preproc_train[column].apply(lambda row: re.sub(\"[\\\\\\\\]\", str(mean_dict[column]), row))\n        preproc_train[column] = preproc_train[column].apply(lambda row: re.sub(\"[[]\", str(mean_dict[column]), row))\n        preproc_train[column] = preproc_train[column].apply(lambda row: re.sub(\"[`]\", \"\", row))\n        preproc_train[column] = preproc_train[column].apply(lambda row: re.sub(\"^[a]$\", str(mean_dict[column]), row))\n        preproc_train[column] = preproc_train[column].apply(lambda row: re.sub(\"^(nan)$\", str(mean_dict[column]), row))\n","execution_count":240,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ubah data ke bentuk float"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in preproc_train:\n    if(column != 'Result'):\n        for i in range(len(preproc_train[column])):\n            try:\n                float(preproc_train[column][i])\n            except:\n                print(column, i, preproc_train[column][i])","execution_count":241,"outputs":[{"output_type":"stream","text":"word-9 571 \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preproc_train.apply(pd.to_numeric, errors='coerce', downcast='float')\nfloat_train = preproc_train.copy()\nfloat_train['word-9'][571] = mean_dict['word-9']","execution_count":242,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_train = float_train.astype(float)\nfloat_train.info()","execution_count":243,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3620 entries, 0 to 3619\nData columns (total 41 columns):\nword-1     3620 non-null float64\nword-2     3620 non-null float64\nword-3     3620 non-null float64\nword-4     3620 non-null float64\nword-5     3620 non-null float64\nword-6     3620 non-null float64\nword-7     3620 non-null float64\nword-8     3620 non-null float64\nword-9     3620 non-null float64\nword-10    3620 non-null float64\nword-11    3620 non-null float64\nword-12    3620 non-null float64\nword-13    3620 non-null float64\nword-14    3620 non-null float64\nword-15    3620 non-null float64\nword-16    3620 non-null float64\nword-17    3620 non-null float64\nword-18    3620 non-null float64\nword-19    3620 non-null float64\nword-20    3620 non-null float64\nword-21    3620 non-null float64\nword-22    3620 non-null float64\nword-23    3620 non-null float64\nword-24    3620 non-null float64\nword-25    3620 non-null float64\nword-26    3620 non-null float64\nword-27    3620 non-null float64\nword-28    3620 non-null float64\nword-29    3620 non-null float64\nword-30    3620 non-null float64\nword-31    3620 non-null float64\nword-32    3620 non-null float64\nword-33    3620 non-null float64\nword-34    3620 non-null float64\nword-35    3620 non-null float64\nword-36    3620 non-null float64\nword-37    3620 non-null float64\nword-38    3620 non-null float64\nword-39    3620 non-null float64\nword-40    3620 non-null float64\nResult     3620 non-null float64\ndtypes: float64(41)\nmemory usage: 1.1 MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_train.head()","execution_count":244,"outputs":[{"output_type":"execute_result","execution_count":244,"data":{"text/plain":"   word-1  word-2  word-3  word-4    word-5  word-6  word-7  word-8  word-9  \\\n0     5.0     0.0     0.0     0.0  0.000000     0.0     0.0     1.0     0.0   \n1   117.0     1.0     4.0     3.0  9.576399     5.0     2.0     9.0    19.0   \n2    30.0     1.0     1.0     5.0  8.000000     3.0     2.0     2.0     2.0   \n3     3.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0     1.0   \n4   263.0     6.0     8.0     8.0  9.576399    26.0     1.0    21.0    35.0   \n\n   word-10  word-11  word-12  word-13  word-14  word-15  word-16  word-17  \\\n0      1.0      1.0      0.0      0.0      1.0      0.0      0.0      0.0   \n1      6.0      1.0      2.0      3.0      9.0     15.0      4.0     10.0   \n2      1.0      1.0      1.0      0.0      1.0      7.0      1.0      1.0   \n3      0.0      0.0      0.0      0.0      1.0      1.0      0.0      0.0   \n4     10.0      5.0      9.0      4.0     14.0     42.0     13.0     15.0   \n\n   word-18     word-19  word-20  word-21  word-22  word-23  word-24  word-25  \\\n0      0.0    1.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n1      9.0   79.000000      1.0      0.0     19.0     17.0      5.0     25.0   \n2      1.0   17.000000      1.0      1.0      7.0      1.0      0.0     11.0   \n3      0.0    1.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n4      6.0  473.704107     20.0      4.0     86.0     19.0     25.0     18.0   \n\n   word-26  word-27  word-28    word-29  word-30  word-31  word-32    word-33  \\\n0      0.0      0.0      0.0   1.000000      0.0      0.0      0.0   0.000000   \n1      0.0      8.0      2.0  10.971976      2.0      1.0      3.0  13.000000   \n2      2.0      1.0      1.0   2.000000      0.0      0.0      1.0   5.000000   \n3      0.0      1.0      0.0   0.000000      0.0      0.0      0.0   6.541817   \n4      5.0     14.0     24.0  56.000000     20.0      6.0     10.0   6.541817   \n\n   word-34  word-35  word-36   word-37    word-38  word-39  word-40  Result  \n0      0.0      0.0      0.0   0.00000    0.00000      0.0      0.0     0.0  \n1      9.0     15.0      3.0   3.24792   13.00000      3.0      1.0     0.0  \n2      1.0      1.0      1.0   0.00000    0.00000      0.0      0.0     0.0  \n3      0.0      0.0      0.0   0.00000  116.54616      0.0      0.0     0.0  \n4      5.0     49.0     26.0  24.00000   23.00000      4.0     24.0     1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word-1</th>\n      <th>word-2</th>\n      <th>word-3</th>\n      <th>word-4</th>\n      <th>word-5</th>\n      <th>word-6</th>\n      <th>word-7</th>\n      <th>word-8</th>\n      <th>word-9</th>\n      <th>word-10</th>\n      <th>word-11</th>\n      <th>word-12</th>\n      <th>word-13</th>\n      <th>word-14</th>\n      <th>word-15</th>\n      <th>word-16</th>\n      <th>word-17</th>\n      <th>word-18</th>\n      <th>word-19</th>\n      <th>word-20</th>\n      <th>word-21</th>\n      <th>word-22</th>\n      <th>word-23</th>\n      <th>word-24</th>\n      <th>word-25</th>\n      <th>word-26</th>\n      <th>word-27</th>\n      <th>word-28</th>\n      <th>word-29</th>\n      <th>word-30</th>\n      <th>word-31</th>\n      <th>word-32</th>\n      <th>word-33</th>\n      <th>word-34</th>\n      <th>word-35</th>\n      <th>word-36</th>\n      <th>word-37</th>\n      <th>word-38</th>\n      <th>word-39</th>\n      <th>word-40</th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>117.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>9.576399</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>19.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>4.0</td>\n      <td>10.0</td>\n      <td>9.0</td>\n      <td>79.000000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>19.0</td>\n      <td>17.0</td>\n      <td>5.0</td>\n      <td>25.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>2.0</td>\n      <td>10.971976</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>13.000000</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>3.0</td>\n      <td>3.24792</td>\n      <td>13.00000</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>8.000000</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>17.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>11.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.541817</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>116.54616</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>263.0</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>9.576399</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>35.0</td>\n      <td>10.0</td>\n      <td>5.0</td>\n      <td>9.0</td>\n      <td>4.0</td>\n      <td>14.0</td>\n      <td>42.0</td>\n      <td>13.0</td>\n      <td>15.0</td>\n      <td>6.0</td>\n      <td>473.704107</td>\n      <td>20.0</td>\n      <td>4.0</td>\n      <td>86.0</td>\n      <td>19.0</td>\n      <td>25.0</td>\n      <td>18.0</td>\n      <td>5.0</td>\n      <td>14.0</td>\n      <td>24.0</td>\n      <td>56.000000</td>\n      <td>20.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>6.541817</td>\n      <td>5.0</td>\n      <td>49.0</td>\n      <td>26.0</td>\n      <td>24.00000</td>\n      <td>23.00000</td>\n      <td>4.0</td>\n      <td>24.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_train.describe()","execution_count":245,"outputs":[{"output_type":"execute_result","execution_count":245,"data":{"text/plain":"              word-1       word-2       word-3       word-4       word-5  \\\ncount    3620.000000  3620.000000  3620.000000  3620.000000  3620.000000   \nmean      218.460643     1.752775     1.298392     1.926606     9.606785   \nstd      9775.426834     3.743087     2.761243     4.118265    16.428094   \nmin         0.000000     0.000000     0.000000    -1.000000     0.000000   \n25%        12.000000     0.000000     0.000000     0.000000     1.000000   \n50%        28.000000     1.000000     0.000000     1.000000     5.000000   \n75%        63.000000     2.000000     1.298392     2.000000    11.000000   \nmax    588184.000000    81.000000    66.000000   117.000000   305.000000   \n\n            word-6      word-7       word-8       word-9      word-10  \\\ncount  3620.000000  3620.00000  3620.000000  3620.000000  3620.000000   \nmean      3.046150     1.41000     4.763116     6.865491     3.248323   \nstd       6.042503     2.58371     7.444187    13.014915     5.071307   \nmin       0.000000     0.00000     0.000000     0.000000     0.000000   \n25%       0.000000     0.00000     1.000000     1.000000     0.000000   \n50%       1.000000     1.00000     3.000000     3.000000     1.000000   \n75%       3.000000     2.00000     6.000000     7.000000     4.000000   \nmax      89.000000    31.00000   135.000000   267.000000    67.000000   \n\n           word-11      word-12      word-13      word-14      word-15  \\\ncount  3620.000000  3620.000000  3620.000000  3620.000000  3620.000000   \nmean      2.194946     1.290278     1.358504     3.083449     9.063404   \nstd       3.760998     2.734632     2.899668     4.563145    14.563384   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     1.000000     1.000000   \n50%       1.000000     0.000000     0.000000     2.000000     4.000000   \n75%       3.000000     1.290278     2.000000     4.000000    10.000000   \nmax      66.000000    56.000000    47.000000    45.000000   275.000000   \n\n           word-16      word-17      word-18        word-19      word-20  \\\ncount  3620.000000  3620.000000  3620.000000    3620.000000  3620.000000   \nmean      1.435578     2.996664     1.485548     473.296116     1.513034   \nstd       2.663253     5.473541     2.995380   18782.227293     4.355978   \nmin       0.000000     0.000000     0.000000       0.000000     0.000000   \n25%       0.000000     0.000000     0.000000       9.000000     0.000000   \n50%       0.000000     1.000000     0.000000      22.000000     0.000000   \n75%       2.000000     3.000000     2.000000      52.000000     2.000000   \nmax      39.000000    83.000000    43.000000  994821.000000   123.000000   \n\n           word-21       word-22      word-23      word-24      word-25  \\\ncount  3620.000000  3.620000e+03  3620.000000  3620.000000  3620.000000   \nmean      1.259352  8.713060e+03     5.324827     4.533095     5.262559   \nstd       2.488839  5.221457e+05     9.001376     8.592310     8.079113   \nmin       0.000000  0.000000e+00     0.000000     0.000000    -9.000000   \n25%       0.000000  1.000000e+00     0.000000     0.000000     1.000000   \n50%       0.000000  5.000000e+00     2.000000     2.000000     3.000000   \n75%       2.000000  1.200000e+01     6.000000     5.000000     6.000000   \nmax      43.000000  3.141567e+07   111.000000   131.000000   108.000000   \n\n           word-26      word-27      word-28      word-29      word-30  \\\ncount  3620.000000  3620.000000  3620.000000  3620.000000  3620.000000   \nmean      1.068755     2.960101     2.631098    10.971976     1.554414   \nstd       3.066138     4.174233     6.290736    18.236360     3.235520   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     1.000000     0.000000     2.000000     0.000000   \n50%       0.000000     2.000000     1.000000     5.000000     0.000000   \n75%       1.000000     4.000000     2.631098    12.000000     2.000000   \nmax      39.000000    62.000000    77.000000   302.000000    37.000000   \n\n           word-31      word-32      word-33      word-34      word-35  \\\ncount  3620.000000  3620.000000  3620.000000  3620.000000  3620.000000   \nmean      1.319202     1.772853     6.545938     1.372125     6.209669   \nstd       3.381691     3.395613    11.573983     2.790326     9.628248   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     1.000000   \n50%       0.000000     1.000000     3.000000     0.000000     3.000000   \n75%       1.000000     2.000000     8.000000     2.000000     7.000000   \nmax      57.000000    66.000000   210.000000    33.000000   132.000000   \n\n             word-36      word-37        word-38      word-39     word-40  \\\ncount    3620.000000  3620.000000    3620.000000  3620.000000  3620.00000   \nmean       37.739950     3.247920     116.516728     1.991699     2.47885   \nstd      2159.732525     6.075976    6853.184123     3.625146     4.47475   \nmin         0.000000     0.000000       0.000000     0.000000     0.00000   \n25%         0.000000     0.000000       0.000000     0.000000     0.00000   \n50%         0.000000     1.000000       1.000000     1.000000     1.00000   \n75%         2.000000     3.061980       3.000000     2.000000     3.00000   \nmax    129945.000000   108.000000  412334.000000    63.000000    70.00000   \n\n            Result  \ncount  3620.000000  \nmean      0.288950  \nstd       0.453337  \nmin       0.000000  \n25%       0.000000  \n50%       0.000000  \n75%       1.000000  \nmax       1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word-1</th>\n      <th>word-2</th>\n      <th>word-3</th>\n      <th>word-4</th>\n      <th>word-5</th>\n      <th>word-6</th>\n      <th>word-7</th>\n      <th>word-8</th>\n      <th>word-9</th>\n      <th>word-10</th>\n      <th>word-11</th>\n      <th>word-12</th>\n      <th>word-13</th>\n      <th>word-14</th>\n      <th>word-15</th>\n      <th>word-16</th>\n      <th>word-17</th>\n      <th>word-18</th>\n      <th>word-19</th>\n      <th>word-20</th>\n      <th>word-21</th>\n      <th>word-22</th>\n      <th>word-23</th>\n      <th>word-24</th>\n      <th>word-25</th>\n      <th>word-26</th>\n      <th>word-27</th>\n      <th>word-28</th>\n      <th>word-29</th>\n      <th>word-30</th>\n      <th>word-31</th>\n      <th>word-32</th>\n      <th>word-33</th>\n      <th>word-34</th>\n      <th>word-35</th>\n      <th>word-36</th>\n      <th>word-37</th>\n      <th>word-38</th>\n      <th>word-39</th>\n      <th>word-40</th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.00000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3.620000e+03</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.00000</td>\n      <td>3620.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>218.460643</td>\n      <td>1.752775</td>\n      <td>1.298392</td>\n      <td>1.926606</td>\n      <td>9.606785</td>\n      <td>3.046150</td>\n      <td>1.41000</td>\n      <td>4.763116</td>\n      <td>6.865491</td>\n      <td>3.248323</td>\n      <td>2.194946</td>\n      <td>1.290278</td>\n      <td>1.358504</td>\n      <td>3.083449</td>\n      <td>9.063404</td>\n      <td>1.435578</td>\n      <td>2.996664</td>\n      <td>1.485548</td>\n      <td>473.296116</td>\n      <td>1.513034</td>\n      <td>1.259352</td>\n      <td>8.713060e+03</td>\n      <td>5.324827</td>\n      <td>4.533095</td>\n      <td>5.262559</td>\n      <td>1.068755</td>\n      <td>2.960101</td>\n      <td>2.631098</td>\n      <td>10.971976</td>\n      <td>1.554414</td>\n      <td>1.319202</td>\n      <td>1.772853</td>\n      <td>6.545938</td>\n      <td>1.372125</td>\n      <td>6.209669</td>\n      <td>37.739950</td>\n      <td>3.247920</td>\n      <td>116.516728</td>\n      <td>1.991699</td>\n      <td>2.47885</td>\n      <td>0.288950</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9775.426834</td>\n      <td>3.743087</td>\n      <td>2.761243</td>\n      <td>4.118265</td>\n      <td>16.428094</td>\n      <td>6.042503</td>\n      <td>2.58371</td>\n      <td>7.444187</td>\n      <td>13.014915</td>\n      <td>5.071307</td>\n      <td>3.760998</td>\n      <td>2.734632</td>\n      <td>2.899668</td>\n      <td>4.563145</td>\n      <td>14.563384</td>\n      <td>2.663253</td>\n      <td>5.473541</td>\n      <td>2.995380</td>\n      <td>18782.227293</td>\n      <td>4.355978</td>\n      <td>2.488839</td>\n      <td>5.221457e+05</td>\n      <td>9.001376</td>\n      <td>8.592310</td>\n      <td>8.079113</td>\n      <td>3.066138</td>\n      <td>4.174233</td>\n      <td>6.290736</td>\n      <td>18.236360</td>\n      <td>3.235520</td>\n      <td>3.381691</td>\n      <td>3.395613</td>\n      <td>11.573983</td>\n      <td>2.790326</td>\n      <td>9.628248</td>\n      <td>2159.732525</td>\n      <td>6.075976</td>\n      <td>6853.184123</td>\n      <td>3.625146</td>\n      <td>4.47475</td>\n      <td>0.453337</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-9.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>12.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>28.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000e+00</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>63.000000</td>\n      <td>2.000000</td>\n      <td>1.298392</td>\n      <td>2.000000</td>\n      <td>11.000000</td>\n      <td>3.000000</td>\n      <td>2.00000</td>\n      <td>6.000000</td>\n      <td>7.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>1.290278</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>10.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>52.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>1.200000e+01</td>\n      <td>6.000000</td>\n      <td>5.000000</td>\n      <td>6.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>2.631098</td>\n      <td>12.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>8.000000</td>\n      <td>2.000000</td>\n      <td>7.000000</td>\n      <td>2.000000</td>\n      <td>3.061980</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>3.00000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>588184.000000</td>\n      <td>81.000000</td>\n      <td>66.000000</td>\n      <td>117.000000</td>\n      <td>305.000000</td>\n      <td>89.000000</td>\n      <td>31.00000</td>\n      <td>135.000000</td>\n      <td>267.000000</td>\n      <td>67.000000</td>\n      <td>66.000000</td>\n      <td>56.000000</td>\n      <td>47.000000</td>\n      <td>45.000000</td>\n      <td>275.000000</td>\n      <td>39.000000</td>\n      <td>83.000000</td>\n      <td>43.000000</td>\n      <td>994821.000000</td>\n      <td>123.000000</td>\n      <td>43.000000</td>\n      <td>3.141567e+07</td>\n      <td>111.000000</td>\n      <td>131.000000</td>\n      <td>108.000000</td>\n      <td>39.000000</td>\n      <td>62.000000</td>\n      <td>77.000000</td>\n      <td>302.000000</td>\n      <td>37.000000</td>\n      <td>57.000000</td>\n      <td>66.000000</td>\n      <td>210.000000</td>\n      <td>33.000000</td>\n      <td>132.000000</td>\n      <td>129945.000000</td>\n      <td>108.000000</td>\n      <td>412334.000000</td>\n      <td>63.000000</td>\n      <td>70.00000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Cek persebaran null value pada data"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = float_train.isnull()\nsns.heatmap(data = missing_values)\nprint(\"jumlah NaN pada data: \",float_train.isnull().sum().sum())","execution_count":246,"outputs":[{"output_type":"stream","text":"jumlah NaN pada data:  0\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"# Handle Missing Value Data Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc_test = test.copy()\npreproc_test.drop('id', axis=1, inplace=True)\npreproc_test = preproc_test.astype('str')\n\nfor column in preproc_test:\n    if(column != 'Result'):\n        preproc_test[column] = preproc_test[column].apply(lambda row: re.sub(\"^(nan)$\", str(mean_dict[column]), row))\n","execution_count":247,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_test = preproc_test.copy()\nfloat_test = float_test.astype(float)\nfloat_test.info()","execution_count":248,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1552 entries, 0 to 1551\nData columns (total 40 columns):\nword-1     1552 non-null float64\nword-2     1552 non-null float64\nword-3     1552 non-null float64\nword-4     1552 non-null float64\nword-5     1552 non-null float64\nword-6     1552 non-null float64\nword-7     1552 non-null float64\nword-8     1552 non-null float64\nword-9     1552 non-null float64\nword-10    1552 non-null float64\nword-11    1552 non-null float64\nword-12    1552 non-null float64\nword-13    1552 non-null float64\nword-14    1552 non-null float64\nword-15    1552 non-null float64\nword-16    1552 non-null float64\nword-17    1552 non-null float64\nword-18    1552 non-null float64\nword-19    1552 non-null float64\nword-20    1552 non-null float64\nword-21    1552 non-null float64\nword-22    1552 non-null float64\nword-23    1552 non-null float64\nword-24    1552 non-null float64\nword-25    1552 non-null float64\nword-26    1552 non-null float64\nword-27    1552 non-null float64\nword-28    1552 non-null float64\nword-29    1552 non-null float64\nword-30    1552 non-null float64\nword-31    1552 non-null float64\nword-32    1552 non-null float64\nword-33    1552 non-null float64\nword-34    1552 non-null float64\nword-35    1552 non-null float64\nword-36    1552 non-null float64\nword-37    1552 non-null float64\nword-38    1552 non-null float64\nword-39    1552 non-null float64\nword-40    1552 non-null float64\ndtypes: float64(40)\nmemory usage: 485.1 KB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Cek Unusual Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"float_train.describe()","execution_count":249,"outputs":[{"output_type":"execute_result","execution_count":249,"data":{"text/plain":"              word-1       word-2       word-3       word-4       word-5  \\\ncount    3620.000000  3620.000000  3620.000000  3620.000000  3620.000000   \nmean      218.460643     1.752775     1.298392     1.926606     9.606785   \nstd      9775.426834     3.743087     2.761243     4.118265    16.428094   \nmin         0.000000     0.000000     0.000000    -1.000000     0.000000   \n25%        12.000000     0.000000     0.000000     0.000000     1.000000   \n50%        28.000000     1.000000     0.000000     1.000000     5.000000   \n75%        63.000000     2.000000     1.298392     2.000000    11.000000   \nmax    588184.000000    81.000000    66.000000   117.000000   305.000000   \n\n            word-6      word-7       word-8       word-9      word-10  \\\ncount  3620.000000  3620.00000  3620.000000  3620.000000  3620.000000   \nmean      3.046150     1.41000     4.763116     6.865491     3.248323   \nstd       6.042503     2.58371     7.444187    13.014915     5.071307   \nmin       0.000000     0.00000     0.000000     0.000000     0.000000   \n25%       0.000000     0.00000     1.000000     1.000000     0.000000   \n50%       1.000000     1.00000     3.000000     3.000000     1.000000   \n75%       3.000000     2.00000     6.000000     7.000000     4.000000   \nmax      89.000000    31.00000   135.000000   267.000000    67.000000   \n\n           word-11      word-12      word-13      word-14      word-15  \\\ncount  3620.000000  3620.000000  3620.000000  3620.000000  3620.000000   \nmean      2.194946     1.290278     1.358504     3.083449     9.063404   \nstd       3.760998     2.734632     2.899668     4.563145    14.563384   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     1.000000     1.000000   \n50%       1.000000     0.000000     0.000000     2.000000     4.000000   \n75%       3.000000     1.290278     2.000000     4.000000    10.000000   \nmax      66.000000    56.000000    47.000000    45.000000   275.000000   \n\n           word-16      word-17      word-18        word-19      word-20  \\\ncount  3620.000000  3620.000000  3620.000000    3620.000000  3620.000000   \nmean      1.435578     2.996664     1.485548     473.296116     1.513034   \nstd       2.663253     5.473541     2.995380   18782.227293     4.355978   \nmin       0.000000     0.000000     0.000000       0.000000     0.000000   \n25%       0.000000     0.000000     0.000000       9.000000     0.000000   \n50%       0.000000     1.000000     0.000000      22.000000     0.000000   \n75%       2.000000     3.000000     2.000000      52.000000     2.000000   \nmax      39.000000    83.000000    43.000000  994821.000000   123.000000   \n\n           word-21       word-22      word-23      word-24      word-25  \\\ncount  3620.000000  3.620000e+03  3620.000000  3620.000000  3620.000000   \nmean      1.259352  8.713060e+03     5.324827     4.533095     5.262559   \nstd       2.488839  5.221457e+05     9.001376     8.592310     8.079113   \nmin       0.000000  0.000000e+00     0.000000     0.000000    -9.000000   \n25%       0.000000  1.000000e+00     0.000000     0.000000     1.000000   \n50%       0.000000  5.000000e+00     2.000000     2.000000     3.000000   \n75%       2.000000  1.200000e+01     6.000000     5.000000     6.000000   \nmax      43.000000  3.141567e+07   111.000000   131.000000   108.000000   \n\n           word-26      word-27      word-28      word-29      word-30  \\\ncount  3620.000000  3620.000000  3620.000000  3620.000000  3620.000000   \nmean      1.068755     2.960101     2.631098    10.971976     1.554414   \nstd       3.066138     4.174233     6.290736    18.236360     3.235520   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     1.000000     0.000000     2.000000     0.000000   \n50%       0.000000     2.000000     1.000000     5.000000     0.000000   \n75%       1.000000     4.000000     2.631098    12.000000     2.000000   \nmax      39.000000    62.000000    77.000000   302.000000    37.000000   \n\n           word-31      word-32      word-33      word-34      word-35  \\\ncount  3620.000000  3620.000000  3620.000000  3620.000000  3620.000000   \nmean      1.319202     1.772853     6.545938     1.372125     6.209669   \nstd       3.381691     3.395613    11.573983     2.790326     9.628248   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     1.000000   \n50%       0.000000     1.000000     3.000000     0.000000     3.000000   \n75%       1.000000     2.000000     8.000000     2.000000     7.000000   \nmax      57.000000    66.000000   210.000000    33.000000   132.000000   \n\n             word-36      word-37        word-38      word-39     word-40  \\\ncount    3620.000000  3620.000000    3620.000000  3620.000000  3620.00000   \nmean       37.739950     3.247920     116.516728     1.991699     2.47885   \nstd      2159.732525     6.075976    6853.184123     3.625146     4.47475   \nmin         0.000000     0.000000       0.000000     0.000000     0.00000   \n25%         0.000000     0.000000       0.000000     0.000000     0.00000   \n50%         0.000000     1.000000       1.000000     1.000000     1.00000   \n75%         2.000000     3.061980       3.000000     2.000000     3.00000   \nmax    129945.000000   108.000000  412334.000000    63.000000    70.00000   \n\n            Result  \ncount  3620.000000  \nmean      0.288950  \nstd       0.453337  \nmin       0.000000  \n25%       0.000000  \n50%       0.000000  \n75%       1.000000  \nmax       1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word-1</th>\n      <th>word-2</th>\n      <th>word-3</th>\n      <th>word-4</th>\n      <th>word-5</th>\n      <th>word-6</th>\n      <th>word-7</th>\n      <th>word-8</th>\n      <th>word-9</th>\n      <th>word-10</th>\n      <th>word-11</th>\n      <th>word-12</th>\n      <th>word-13</th>\n      <th>word-14</th>\n      <th>word-15</th>\n      <th>word-16</th>\n      <th>word-17</th>\n      <th>word-18</th>\n      <th>word-19</th>\n      <th>word-20</th>\n      <th>word-21</th>\n      <th>word-22</th>\n      <th>word-23</th>\n      <th>word-24</th>\n      <th>word-25</th>\n      <th>word-26</th>\n      <th>word-27</th>\n      <th>word-28</th>\n      <th>word-29</th>\n      <th>word-30</th>\n      <th>word-31</th>\n      <th>word-32</th>\n      <th>word-33</th>\n      <th>word-34</th>\n      <th>word-35</th>\n      <th>word-36</th>\n      <th>word-37</th>\n      <th>word-38</th>\n      <th>word-39</th>\n      <th>word-40</th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.00000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3.620000e+03</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.000000</td>\n      <td>3620.00000</td>\n      <td>3620.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>218.460643</td>\n      <td>1.752775</td>\n      <td>1.298392</td>\n      <td>1.926606</td>\n      <td>9.606785</td>\n      <td>3.046150</td>\n      <td>1.41000</td>\n      <td>4.763116</td>\n      <td>6.865491</td>\n      <td>3.248323</td>\n      <td>2.194946</td>\n      <td>1.290278</td>\n      <td>1.358504</td>\n      <td>3.083449</td>\n      <td>9.063404</td>\n      <td>1.435578</td>\n      <td>2.996664</td>\n      <td>1.485548</td>\n      <td>473.296116</td>\n      <td>1.513034</td>\n      <td>1.259352</td>\n      <td>8.713060e+03</td>\n      <td>5.324827</td>\n      <td>4.533095</td>\n      <td>5.262559</td>\n      <td>1.068755</td>\n      <td>2.960101</td>\n      <td>2.631098</td>\n      <td>10.971976</td>\n      <td>1.554414</td>\n      <td>1.319202</td>\n      <td>1.772853</td>\n      <td>6.545938</td>\n      <td>1.372125</td>\n      <td>6.209669</td>\n      <td>37.739950</td>\n      <td>3.247920</td>\n      <td>116.516728</td>\n      <td>1.991699</td>\n      <td>2.47885</td>\n      <td>0.288950</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9775.426834</td>\n      <td>3.743087</td>\n      <td>2.761243</td>\n      <td>4.118265</td>\n      <td>16.428094</td>\n      <td>6.042503</td>\n      <td>2.58371</td>\n      <td>7.444187</td>\n      <td>13.014915</td>\n      <td>5.071307</td>\n      <td>3.760998</td>\n      <td>2.734632</td>\n      <td>2.899668</td>\n      <td>4.563145</td>\n      <td>14.563384</td>\n      <td>2.663253</td>\n      <td>5.473541</td>\n      <td>2.995380</td>\n      <td>18782.227293</td>\n      <td>4.355978</td>\n      <td>2.488839</td>\n      <td>5.221457e+05</td>\n      <td>9.001376</td>\n      <td>8.592310</td>\n      <td>8.079113</td>\n      <td>3.066138</td>\n      <td>4.174233</td>\n      <td>6.290736</td>\n      <td>18.236360</td>\n      <td>3.235520</td>\n      <td>3.381691</td>\n      <td>3.395613</td>\n      <td>11.573983</td>\n      <td>2.790326</td>\n      <td>9.628248</td>\n      <td>2159.732525</td>\n      <td>6.075976</td>\n      <td>6853.184123</td>\n      <td>3.625146</td>\n      <td>4.47475</td>\n      <td>0.453337</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-9.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>12.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>28.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000e+00</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>63.000000</td>\n      <td>2.000000</td>\n      <td>1.298392</td>\n      <td>2.000000</td>\n      <td>11.000000</td>\n      <td>3.000000</td>\n      <td>2.00000</td>\n      <td>6.000000</td>\n      <td>7.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>1.290278</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>10.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>52.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>1.200000e+01</td>\n      <td>6.000000</td>\n      <td>5.000000</td>\n      <td>6.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>2.631098</td>\n      <td>12.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>8.000000</td>\n      <td>2.000000</td>\n      <td>7.000000</td>\n      <td>2.000000</td>\n      <td>3.061980</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>3.00000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>588184.000000</td>\n      <td>81.000000</td>\n      <td>66.000000</td>\n      <td>117.000000</td>\n      <td>305.000000</td>\n      <td>89.000000</td>\n      <td>31.00000</td>\n      <td>135.000000</td>\n      <td>267.000000</td>\n      <td>67.000000</td>\n      <td>66.000000</td>\n      <td>56.000000</td>\n      <td>47.000000</td>\n      <td>45.000000</td>\n      <td>275.000000</td>\n      <td>39.000000</td>\n      <td>83.000000</td>\n      <td>43.000000</td>\n      <td>994821.000000</td>\n      <td>123.000000</td>\n      <td>43.000000</td>\n      <td>3.141567e+07</td>\n      <td>111.000000</td>\n      <td>131.000000</td>\n      <td>108.000000</td>\n      <td>39.000000</td>\n      <td>62.000000</td>\n      <td>77.000000</td>\n      <td>302.000000</td>\n      <td>37.000000</td>\n      <td>57.000000</td>\n      <td>66.000000</td>\n      <td>210.000000</td>\n      <td>33.000000</td>\n      <td>132.000000</td>\n      <td>129945.000000</td>\n      <td>108.000000</td>\n      <td>412334.000000</td>\n      <td>63.000000</td>\n      <td>70.00000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_train['word-1'].value_counts().sort_index()","execution_count":250,"outputs":[{"output_type":"execute_result","execution_count":250,"data":{"text/plain":"0.0          39\n1.0          26\n2.0         118\n3.0         160\n4.0          64\n           ... \n911.0         1\n951.0         1\n1450.0        1\n1898.0        1\n588184.0      1\nName: word-1, Length: 329, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_train[float_train['word-1'] == 588184.0 ]","execution_count":251,"outputs":[{"output_type":"execute_result","execution_count":251,"data":{"text/plain":"        word-1  word-2  word-3  word-4  word-5  word-6  word-7  word-8  \\\n3609  588184.0     0.0     2.0     3.0    15.0     5.0     4.0    12.0   \n\n      word-9  word-10  word-11  word-12  word-13  word-14  word-15  word-16  \\\n3609     5.0      3.0      3.0      4.0      6.0      3.0     19.0      3.0   \n\n      word-17  word-18  word-19  word-20  word-21  word-22  word-23  word-24  \\\n3609      3.0      3.0     44.0      5.0      4.0      2.0      6.0      4.0   \n\n      word-25  word-26  word-27  word-28  word-29  word-30  word-31  word-32  \\\n3609      8.0      0.0      6.0      1.0     14.0      0.0      1.0      3.0   \n\n      word-33  word-34  word-35  word-36  word-37  word-38  word-39  word-40  \\\n3609      7.0      3.0      6.0      3.0      0.0      0.0      5.0      7.0   \n\n      Result  \n3609     0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word-1</th>\n      <th>word-2</th>\n      <th>word-3</th>\n      <th>word-4</th>\n      <th>word-5</th>\n      <th>word-6</th>\n      <th>word-7</th>\n      <th>word-8</th>\n      <th>word-9</th>\n      <th>word-10</th>\n      <th>word-11</th>\n      <th>word-12</th>\n      <th>word-13</th>\n      <th>word-14</th>\n      <th>word-15</th>\n      <th>word-16</th>\n      <th>word-17</th>\n      <th>word-18</th>\n      <th>word-19</th>\n      <th>word-20</th>\n      <th>word-21</th>\n      <th>word-22</th>\n      <th>word-23</th>\n      <th>word-24</th>\n      <th>word-25</th>\n      <th>word-26</th>\n      <th>word-27</th>\n      <th>word-28</th>\n      <th>word-29</th>\n      <th>word-30</th>\n      <th>word-31</th>\n      <th>word-32</th>\n      <th>word-33</th>\n      <th>word-34</th>\n      <th>word-35</th>\n      <th>word-36</th>\n      <th>word-37</th>\n      <th>word-38</th>\n      <th>word-39</th>\n      <th>word-40</th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3609</th>\n      <td>588184.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>15.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>12.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>19.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>44.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Split Data Training:Validasi -> 80:20"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_training = float_train[0:int(3620*0.8)].copy()\ndata_validation = float_train[int(3620*0.8):int(3620*0.8)+int(3620*0.2)].copy()\ndata_cv = float_train.copy()","execution_count":252,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = float_train.columns.tolist()\n\nx_train = data_training[cols[0:40]]\ny_train = data_training[cols[40]]\n\nx_val = data_validation[cols[0:40]]\ny_val = data_validation[cols[40]]\n\nx_cv = data_cv[cols[0:40]]\ny_cv = data_cv[cols[40]]\n\n","execution_count":253,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nscaler.fit(x_cv)\nscaled_x_cv = scaler.transform(x_cv)\nscaled_x_test = scaler.transform(float_test)\nscaled_x_train = scaler.transform(x_train)\nscaled_x_val = scaler.transform(x_val)","execution_count":254,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_x_test = scaler.transform(float_test)","execution_count":255,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"label = y_cv.copy()","execution_count":256,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=2)\nfeatures, label = sm.fit_sample(scaled_x_cv , label.ravel())","execution_count":257,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\ncollections.Counter(label)","execution_count":258,"outputs":[{"output_type":"execute_result","execution_count":258,"data":{"text/plain":"Counter({0.0: 2574, 1.0: 2574})"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.var(scaled_x_val, axis=0)\nfor i in range(len(x)):\n    print(x[i])","execution_count":259,"outputs":[{"output_type":"stream","text":"4.994142770992758\n1.5636096567566935\n0.9044152982108559\n2.19635194191902\n1.3746580450189019\n1.0759530896542986\n0.9162712534101309\n1.1392439917094543\n1.6290487954326092\n1.1090464225555319\n1.1782717103010873\n1.8814781132973364\n1.4937334916173501\n1.074604588963217\n1.5460148328581786\n1.443762647845895\n1.2958994407234858\n1.2461197000484288\n2.6314581317069737e-05\n2.8317196462114205\n1.2353707849695872\n3.850833721207318e-07\n1.3758392715486092\n1.4878533052388163\n1.2354578090005168\n1.1171384902553754\n1.0750223743314384\n1.1750103184932383\n1.4193693987994198\n1.0462097409761593\n1.681919371762308\n1.4128120426981905\n1.281793195719541\n1.186803165834364\n1.2228579383201756\n4.994468771702662\n1.296630899032544\n8.959410795050881e-07\n1.070178336013083\n1.2886472212421012\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build model Sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom xgboost.sklearn import XGBClassifier\n\n\nfrom sklearn.model_selection import cross_val_score","execution_count":260,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfmodel = RandomForestClassifier(n_estimators=2000, criterion='gini', n_jobs=-1)\nstart=datetime.now()  \nscore =  cross_val_score(rfmodel, x_cv, y_cv, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":261,"outputs":[{"output_type":"stream","text":"0:00:35.091143\nscore:  0.8828729281767955\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(rfmodel, scaled_x_cv, y_cv, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":262,"outputs":[{"output_type":"stream","text":"0:00:34.070503\nscore:  0.8837016574585637\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(rfmodel, features, label, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":263,"outputs":[{"output_type":"stream","text":"0:00:49.453585\nscore:  0.9271671053997188\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"adaboostmodel = AdaBoostClassifier(n_estimators=5500, random_state=101, learning_rate = 0.1)\nstart=datetime.now()  \nscore =  cross_val_score(adaboostmodel, x_cv, y_cv, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n\nstart=datetime.now()  \nscore =  cross_val_score(adaboostmodel, scaled_x_cv, y_cv, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":264,"outputs":[{"output_type":"stream","text":"0:03:18.312678\nscore:  0.8712707182320442\n0:03:20.064885\nscore:  0.8709944751381216\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(adaboostmodel, features, label, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":265,"outputs":[{"output_type":"stream","text":"0:05:38.682772\nscore:  0.9036674309113384\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbmodel = GradientBoostingClassifier(\n    loss = 'exponential',\n    n_estimators=3000, \n    random_state=101, #261\n    max_features = 2,\n    max_depth = 8\n)\nstart=datetime.now()  \nscore =  cross_val_score(gbmodel, x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(gbmodel, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(gbmodel, features, label, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(\n        kernel=\"rbf\",\n        gamma = 0.0001,\n        C = 1000\n    )\n\nstart=datetime.now()  \nscore =  cross_val_score(svm, x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(\n    kernel=\"rbf\",\n    gamma = 0.101,\n    C = 10\n)\n        \nstart=datetime.now()  \nscore =  cross_val_score(svm, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(svm, features, label, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel = LogisticRegression(max_iter = 100000)\n\nstart=datetime.now()  \nscore =  cross_val_score(logmodel, x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(logmodel, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(logmodel, features, label, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive-Bayes (Bernoulli)"},{"metadata":{"trusted":true},"cell_type":"code","source":"nbmodel = BernoulliNB()\n\nstart=datetime.now()  \nscore =  cross_val_score(nbmodel, x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(nbmodel, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.now()  \nscore =  cross_val_score(nbmodel, features, label, cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best Classical Method : GBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbmodel.fit(features, label)\n\ngbprediction = gbmodel.predict(scaled_x_test)\ngbprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_id = np.arange(3621,5173)\ndata = {\n    'id': data_id,\n    'Result': gbprediction\n}\n\ndf = pd.DataFrame (data, columns = ['id','Result'])\ndf = df.astype(int)\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('early-tori_gb_smote.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting Classifier (GB XGB)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier \n\nestimator = [] \nestimator.append(\n    ('GB1', \n         GradientBoostingClassifier(\n            loss = 'exponential',\n            n_estimators=3000, \n            random_state=261,\n            max_features = 2,\n            max_depth = 8\n        )\n    )\n) \n \n\n\nestimator.append(('XGB', XGBClassifier(\n    learning_rate =0.07,\n    n_estimators=1000,\n    seed=1\n)))\n \n\nvchmodel = VotingClassifier(estimators = estimator, voting ='hard') \nvcsmodel = VotingClassifier(estimators = estimator, voting ='soft') \n\nstart=datetime.now()  \nscore =  cross_val_score(vchmodel, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n\n\nstart=datetime.now()  \nscore =  cross_val_score(vcsmodel, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier \n\nestimator = [] \nestimator.append(\n    ('GB1', \n         GradientBoostingClassifier(\n            loss = 'exponential',\n            n_estimators=3000, \n            random_state=261,\n            max_features = 2,\n            max_depth = 8\n        )\n    )\n) \n \n\n\nestimator.append(('XGB', XGBClassifier(\n    learning_rate =0.07,\n    n_estimators=1000,\n    seed=1\n)))\n \n\nvchmodel = VotingClassifier(estimators = estimator, voting ='hard') \nvcsmodel = VotingClassifier(estimators = estimator, voting ='soft') \n\nstart=datetime.now()  \nscore =  cross_val_score(vchmodel, features, label,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n\n\nstart=datetime.now()  \nscore =  cross_val_score(vcsmodel, features, label,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Voting Classifier (SVM GB RF)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier \n\nestimator = [] \nestimator.append(\n    ('SVC', \n         SVC(\n            kernel=\"rbf\",\n            gamma = 0.101,\n            C = 10,\n            probability = True\n        )\n    )\n)\n\nestimator.append(\n    ('GB', \n         GradientBoostingClassifier(\n            loss = 'exponential',\n            n_estimators=3000, \n            random_state=261,\n            max_features = 2,\n            max_depth = 8\n        )\n    )\n) \n\nestimator.append(\n    ('RF', \n         RandomForestClassifier(n_estimators=2000, criterion='gini')\n    )\n) \n\n\n\n\n\nvchmodel = VotingClassifier(estimators = estimator, voting ='hard') \nvcsmodel = VotingClassifier(estimators = estimator, voting ='soft') \n\nstart=datetime.now()  \nscore =  cross_val_score(vchmodel, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n\n\nstart=datetime.now()  \nscore =  cross_val_score(vcsmodel, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier \n\nestimator = [] \nestimator.append(\n    ('SVC', \n         SVC(\n            kernel=\"rbf\",\n            gamma = 0.101,\n            C = 10,\n            probability = True\n        )\n    )\n)\n\nestimator.append(\n    ('GB', \n         GradientBoostingClassifier(\n            loss = 'exponential',\n            n_estimators=3000, \n            random_state=261,\n            max_features = 2,\n            max_depth = 8\n        )\n    )\n) \n\nestimator.append(\n    ('RF', \n         RandomForestClassifier(n_estimators=2000, criterion='gini')\n    )\n) \n\n\n\n\n\nvchmodel = VotingClassifier(estimators = estimator, voting ='hard') \nvcsmodel = VotingClassifier(estimators = estimator, voting ='soft') \n\nstart=datetime.now()  \nscore =  cross_val_score(vchmodel, features, label,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n\n\nstart=datetime.now()  \nscore =  cross_val_score(vcsmodel,  features, label,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vcsmodel.fit(features, label)\n\nvcsprediction = vcsmodel.predict(scaled_x_test)\n\ndata_id = np.arange(3621,5173)\ndata = {\n    'id': data_id,\n    'Result': vcsprediction\n}\n\ndf = pd.DataFrame (data, columns = ['id','Result'])\ndf = df.astype(int)\ndf.to_csv('early-tori_ensem_ssmote.csv',index=False)\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vchmodel.fit(features, label)\n\nvchprediction = vchmodel.predict(scaled_x_test)\n\ndata_id = np.arange(3621,5173)\ndata = {\n    'id': data_id,\n    'Result': vchprediction\n}\n\ndf = pd.DataFrame (data, columns = ['id','Result'])\ndf = df.astype(int)\ndf.to_csv('early-tori_ensem_hsmote.csv',index=False)\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nxgb_clf = XGBClassifier(\n    learning_rate =0.07,\n    n_estimators=1000,\n    seed=1,\n    max_depth = 6\n)\n\nstart=datetime.now()  \nscore =  cross_val_score(xgb_clf, scaled_x_cv, y_cv,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nxgb_clf = XGBClassifier(\n    learning_rate =0.07,\n    n_estimators=1000,\n    seed=1,\n    max_depth = 6\n)\n\nstart=datetime.now()  \nscore =  cross_val_score(xgb_clf,features, label,cv=5)\nprint(datetime.now()-start)\nprint(\"score: \",score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keras Neural Network Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Convolution1D, MaxPooling1D, Input, Flatten\nfrom keras.optimizers import SGD\nfrom keras.optimizers import RMSprop\nfrom keras.optimizers import Nadam\nfrom keras.optimizers import Adadelta\nfrom keras.utils import to_categorical\n\nctg_y_train = to_categorical(y_train)\nctg_y_val = to_categorical(y_val)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multi-layered Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(1024, activation='relu', input_shape=(40,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(\n    optimizer= Nadam(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nstart=datetime.now()\nhistory = model.fit(\n    scaled_x_cv[0:int(3620*0.8)], ctg_y_train,\n    epochs=150,\n    validation_data=( scaled_x_cv[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val)\n)\nprint(datetime.now()-start)\n\nscore = model.evaluate( scaled_x_cv[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val, verbose=0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(1024, activation='relu', input_shape=(40,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(\n    optimizer= Nadam(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nstart=datetime.now()\nhistory = model.fit(\n    scaled_x_cv[0:int(3620*0.8)], ctg_y_train,\n    epochs=150,\n    validation_data=( scaled_x_cv[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val)\n)\nprint(datetime.now()-start)\n\nscore = model.evaluate( scaled_x_cv[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val, verbose=0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convolutional Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cnn  = scaled_x_cv.reshape(scaled_x_cv.shape[0],40,1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cnn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cnn[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def train_model(conv_layer_count, feature_map):\ninput_layer = Input(\n    shape= (40, 1),\n    name='input'\n)\n\ntemp_layer = input_layer\n    \nfor i in range(2):\n    convolutional_layer = Convolution1D(\n        kernel_size=4, \n        filters=2048,\n        activation='relu',\n        kernel_initializer='glorot_uniform'\n    )(temp_layer)\n        \n    temp_layer = convolutional_layer\n    \nmax_pool = MaxPooling1D(2)(temp_layer)\n    \nflatten = Flatten()(max_pool)\n\ndense = Dense(\n        2048,\n        activation = 'relu'\n    )(flatten)\n\ndropout = Dropout(0.5)(dense)\n    \ndense2 = Dense(\n        1024,\n        activation = 'relu'\n    )(dropout)\n\ndropout2 = Dropout(0.5)(dense2)\n\ndense3 = Dense(\n        512,\n        activation = 'relu'\n    )(dropout2)\n\ndropout3 = Dropout(0.5)(dense3)\n\noutput = Dense(2, activation='softmax')(dropout3)\n\nmodel = Model(inputs=[input_layer], outputs=[output])\n    \nmodel.compile(\n    optimizer= Nadam(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n    \nmodel.summary()\n\nstart=datetime.now()\nhistory = model.fit(\n    x_cnn[0:int(3620*0.8)], ctg_y_train,\n    epochs=150,\n    validation_data=( x_cnn[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val)\n)\nprint(datetime.now()-start)\n\nscore = model.evaluate(x_cnn[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val, verbose=0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss: 0.0078 - accuracy: 0.9965 - val_loss: 3.5941 - val_accuracy: 0.8798","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def train_model(conv_layer_count, feature_map):\ninput_layer = Input(\n    shape= (40, 1),\n    name='input'\n)\n\ntemp_layer = input_layer\n    \nfor i in range(2):\n    convolutional_layer = Convolution1D(\n        kernel_size=8, \n        filters=2048,\n        activation='relu',\n        kernel_initializer='glorot_uniform'\n    )(temp_layer)\n        \n    temp_layer = convolutional_layer\n    \nmax_pool = MaxPooling1D(2)(temp_layer)\n    \nflatten = Flatten()(max_pool)\n\ndense = Dense(\n        2048,\n        activation = 'relu'\n    )(flatten)\n\ndropout = Dropout(0.5)(dense)\n    \ndense2 = Dense(\n        1024,\n        activation = 'relu'\n    )(dropout)\n\ndropout2 = Dropout(0.5)(dense2)\n\ndense3 = Dense(\n        512,\n        activation = 'relu'\n    )(dropout2)\n\ndropout3 = Dropout(0.5)(dense3)\n\noutput = Dense(2, activation='softmax')(dropout3)\n\nmodel = Model(inputs=[input_layer], outputs=[output])\n    \nmodel.compile(\n    optimizer= Nadam(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n    \nmodel.summary()\n\nstart=datetime.now()\nhistory = model.fit(\n    x_cnn[0:int(3620*0.8)], ctg_y_train,\n    epochs=150,\n    validation_data=( x_cnn[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val)\n)\nprint(datetime.now()-start)\n\nscore = model.evaluate(x_cnn[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val, verbose=0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def train_model(conv_layer_count, feature_map):\ninput_layer = Input(\n    shape= (40, 1),\n    name='input'\n)\n\ntemp_layer = input_layer\n    \nfor i in range(2):\n    convolutional_layer = Convolution1D(\n        kernel_size=12, \n        filters=2048,\n        activation='relu',\n        kernel_initializer='glorot_uniform'\n    )(temp_layer)\n        \n    temp_layer = convolutional_layer\n    \nmax_pool = MaxPooling1D(2)(temp_layer)\n    \nflatten = Flatten()(max_pool)\n\ndense = Dense(\n        2048,\n        activation = 'relu'\n    )(flatten)\n\ndropout = Dropout(0.5)(dense)\n    \ndense2 = Dense(\n        1024,\n        activation = 'relu'\n    )(dropout)\n\ndropout2 = Dropout(0.5)(dense2)\n\ndense3 = Dense(\n        512,\n        activation = 'relu'\n    )(dropout2)\n\ndropout3 = Dropout(0.5)(dense3)\n\noutput = Dense(2, activation='softmax')(dropout3)\n\nmodel = Model(inputs=[input_layer], outputs=[output])\n    \nmodel.compile(\n    optimizer= Nadam(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n    \nmodel.summary()\n\nstart=datetime.now()\nhistory = model.fit(\n    x_cnn[0:int(3620*0.8)], ctg_y_train,\n    epochs=150,\n    validation_data=( x_cnn[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val)\n)\nprint(datetime.now()-start)\n\nscore = model.evaluate(x_cnn[int(3620*0.8):int(3620*0.8)+int(3620*0.2)], ctg_y_val, verbose=0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_x_test  = scaled_x_test.reshape(scaled_x_test.shape[0],40,1)\npredict_mlp = model.predict(scaled_x_test)\npmlp = np.argmax(predict_mlp, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_id = np.arange(3621,5173)\ndata = {\n    'id': data_id,\n    'Result': pmlp\n}\n\ndf = pd.DataFrame (data, columns = ['id','Result'])\ndf = df.astype(int)\ndf.head()\ndf.to_csv('pred_mlp.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}